{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "XA0TUiYXqVG6",
        "1_V7TkgfqVHB",
        "UU4dcjSkqVHB",
        "Z-hykwinpx6N",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JitinSaxenaa/Flipkart-ML-Project/blob/main/Copy_of_Sample_ML_Submission_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Flipkart ML Project\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "Name - Jitin Saxena\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project analyzes customer support data from Flipkart to understand service performance, customer feedback patterns, and satisfaction indicators. The dataset consists of 85,000+ support interactions including categorical and text data such as channel_name, category, Sub-category, agent details, timestamps, and Customer Remarks, along with the final CSAT Score rated by customers.\n",
        "\n",
        "The main goal of this project is to derive key insights from the data and build models that can predict customer satisfaction, detect inefficiencies, and identify areas of improvement. The EDA (Exploratory Data Analysis) includes univariate, bivariate, and multivariate analysis to uncover relationships between agent performance, shifts, tenure buckets, issue categories, and customer satisfaction.\n",
        "\n",
        "Missing data is also analyzed and appropriately handled. Textual data like customer remarks are cleaned, preprocessed, and vectorized to extract sentiment or pain points. We apply machine learning models such as Random Forest, Logistic Regression, and XGBoost for CSAT score prediction, with metrics like accuracy, F1-score, and confusion matrix to evaluate performance.\n",
        "\n",
        "Further, hypothesis testing is applied to validate assumptions about agent shift efficiency and issue type impact on satisfaction. Charts and visualizations provide storytelling insights such as response delay effects, shift-wise satisfaction rates, and category-wise issue frequencies.\n",
        "\n",
        "The final model is saved for deployment and can be used for real-time CSAT prediction or support monitoring. This capstone provides valuable business insights to improve service quality and enhance customer retention."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/JitinSaxenaa/Flipkart-ML-Project/tree/main"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flipkart, a leading e-commerce platform, handles a high volume of customer queries through its support channels. To improve customer retention and satisfaction, it's critical to analyze these interactions and predict the factors contributing to poor or excellent support experiences. This project aims to:\n",
        "\n",
        " - nalyze large-scale customer support data for operational insights\n",
        "\n",
        " - Identify variables affecting the CSAT score\n",
        "\n",
        " - Predict CSAT using classification models\n",
        "\n",
        " - Utilize customer remarks (text) for sentiment analysis and issue detection\n",
        "\n",
        " - Recommend actionable strategies to enhance customer service efficiency"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import missingno as msno\n",
        "# Set styles\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv(\"Customer_support_data.csv\")"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(\"Total Rows:\", df.shape[0])\n",
        "print(\"Total Columns:\", df.shape[1])"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "try:\n",
        "    duplicate_count = df.duplicated().sum()\n",
        "    print(f\"\\n🔁 Duplicate Rows: {duplicate_count}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error checking duplicates: {e}\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "try:\n",
        "    print(\"\\n❌ Missing Values per Column:\\n\", df.isnull().sum())\n",
        "    msno.matrix(df.sample(1000))\n",
        "    plt.title(\"Missing Values Matrix\")\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error visualizing missing values: {e}\")"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "msno.matrix(df.sample(1000))  # limit to 1000 rows for visibility"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains customer support call records at Flipkart, including channel types, issue categories, timestamps, and satisfaction ratings. While categorical and timestamp fields are rich in information, several value columns (especially price and handling time) contain significant missing data. These need to be handled before modeling."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(\"\\n📋 Dataset Columns:\\n\", df.columns.tolist())"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "print(\"\\n📊 Dataset Description:\\n\", df.describe(include='all'))\n",
        "print(\"\\n🔢 Unique Values in Each Column:\")\n",
        "for col in df.columns:\n",
        "    print(f\"{col}: {df[col].nunique()} unique values\")"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Column Name              | Description                                                                 |\n",
        "|--------------------------|-----------------------------------------------------------------------------|\n",
        "| Unique id                | Unique identifier for each support interaction                             |\n",
        "| channel_name             | Type of support channel (Inbound, Outcall, etc.)                            |\n",
        "| category                 | Broad category of customer issue                                            |\n",
        "| Sub-category             | Specific sub-category under the main issue category                        |\n",
        "| Customer Remarks         | Feedback or comments provided by the customer                               |\n",
        "| Order_id                 | Associated order ID for the complaint                                       |\n",
        "| order_date_time          | Original order placement date and time                                     |\n",
        "| Issue_reported at        | Timestamp when the issue was reported                                       |\n",
        "| issue_responded          | Timestamp when the issue was responded to                                   |\n",
        "| Survey_response_Date     | Date when the CSAT survey was completed                                     |\n",
        "| Customer_City            | Customer's city of residence                                                |\n",
        "| Product_category         | Category of the product involved                                            |\n",
        "| Item_price               | Price of the item associated with the issue                                |\n",
        "| connected_handling_time  | Duration of agent's interaction with the customer (in seconds/minutes)     |\n",
        "| Agent_name               | Name of the customer support agent handling the issue                       |\n",
        "| Supervisor               | Supervisor under whom the agent reports                                     |\n",
        "| Manager                  | Manager responsible for the team                                            |\n",
        "| Tenure Bucket            | Agent’s experience bucket (e.g., On Job Training, 0–30 days, >90 days)      |\n",
        "| Agent Shift              | Shift during which the agent handled the call (Morning, Evening, etc.)      |\n",
        "| CSAT Score               | Customer Satisfaction rating (usually 1–5)                                  |\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "print(\"\\n🔢 Unique Values for each column:\")\n",
        "for col in df.columns:\n",
        "    print(f\"{col}: {df[col].nunique()} unique values\")\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "try:\n",
        "    # Convert date columns\n",
        "    df['Issue_reported at'] = pd.to_datetime(df['Issue_reported at'], errors='coerce')\n",
        "    df['issue_responded'] = pd.to_datetime(df['issue_responded'], errors='coerce')\n",
        "    df['Survey_response_Date'] = pd.to_datetime(df['Survey_response_Date'], errors='coerce')\n",
        "\n",
        "    # Calculate response time in minutes\n",
        "    df['response_time_mins'] = (df['issue_responded'] - df['Issue_reported at']).dt.total_seconds() / 60\n",
        "\n",
        "    # Drop columns with >80% missing data\n",
        "    df = df.loc[:, df.isnull().mean() < 0.8]\n",
        "\n",
        "    # Handle missing numerical and categorical data\n",
        "    if 'Item_price' in df.columns:\n",
        "        df['Item_price'].fillna(df['Item_price'].median(), inplace=True)\n",
        "    if 'Product_category' in df.columns:\n",
        "        df['Product_category'].fillna('Unknown', inplace=True)\n",
        "\n",
        "    # Drop rows missing target column\n",
        "    df.dropna(subset=['CSAT Score'], inplace=True)\n",
        "\n",
        "    print(f\"\\n✅ Data cleaned. New shape: {df.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error during data wrangling: {e}\")\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Date Conversion\n",
        "2. Response Time Feature Creation\n",
        "3. Dropped High-Null Columns\n",
        "4. Imputed Missing Values"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SUzDJUI4qUpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "VTNBqQ3oqVG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "5zerSXLBqVG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "try:\n",
        "    sns.countplot(data=df, x='channel_name')\n",
        "    plt.title(\"Issue Volume by Channel\")\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"❌ Chart 1 failed:\", e)\n"
      ],
      "metadata": {
        "id": "urISQxSPqVG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "5TxhAUfFqVG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected the pair plot because it allows visualization of pairwise relationships between multiple numerical variables. It helps understand how these variables correlate with each other through scatterplots and distributions, providing a holistic view of the data interactions."
      ],
      "metadata": {
        "id": "LGP48hUzqVG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "28V8deh_qVG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pair plot reveals whether there are correlations between product price, customer service response time, and customer satisfaction (CSAT score). For example, a negative correlation between response time and CSAT score would indicate that faster service leads to higher satisfaction. Similarly, any trend between item price and satisfaction could indicate how pricing affects customer perception."
      ],
      "metadata": {
        "id": "zIumAhspqVG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Yes, understanding these relationships enables Flipkart to optimize operational aspects like response times and pricing strategy to enhance customer satisfaction. If the data shows longer response times lead to lower CSAT, then reducing delays can positively impact retention and sales. If pricing negatively affects satisfaction, this insight can guide more competitive pricing to avoid negative growth."
      ],
      "metadata": {
        "id": "9S9iMXy1qVG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "i3jZYqatqVG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "Hz0LdvtYqVG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "try:\n",
        "    avg_csat = df.groupby('Product_category')['CSAT Score'].mean().reset_index()\n",
        "    sns.barplot(data=avg_csat, x='Product_category', y='CSAT Score')\n",
        "    plt.title(\"Average Customer Satisfaction Score by Product Category\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"❌ Chart 2 failed:\", e)\n"
      ],
      "metadata": {
        "id": "O14r91emqVG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "XA0TUiYXqVG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Countplot clearly shows how many issues each product category has, helping identify categories with higher complaint rates."
      ],
      "metadata": {
        "id": "52jp6pf3qVG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "yzhLmuYJqVG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certain categories have notably higher issue counts, signaling possible product or service quality concerns in those areas."
      ],
      "metadata": {
        "id": "iRnbh0_1qVG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Yes, focusing on problem categories can reduce returns and complaints, improving customer satisfaction. Ignoring this could lead to continued negative feedback affecting brand reputation."
      ],
      "metadata": {
        "id": "boklqydsqVG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Qfzf_7ZaqVG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "hZtLXDf5qVG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "try:\n",
        "    df['order_month'] = pd.to_datetime(df['order_date_time']).dt.month\n",
        "    sns.countplot(data=df, x='order_month')\n",
        "    plt.title(\"Order Volume by Month\")\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"❌ Chart 3 failed:\", e)"
      ],
      "metadata": {
        "id": "1P9KoVcOqVG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)\n",
        "df.columns = df.columns.str.strip()"
      ],
      "metadata": {
        "id": "5oSnlePEhvCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "95WQuzUHqVG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lineplot is suitable for showing how sales change over time, helping identify seasonality or growth patterns."
      ],
      "metadata": {
        "id": "ptmJqBJuqVG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "U1aLPegtqVG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sales peak during festival seasons and dip in off months, indicating strong seasonal trends."
      ],
      "metadata": {
        "id": "K-0KlQhxqVG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Yes, planning inventory and marketing for peak periods will maximize sales. Failing to do so can result in missed revenue opportunities."
      ],
      "metadata": {
        "id": "6A9-My-FqVG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "L4BcR_hpqVG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "Y5vYVkaEqVG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Corrected plotting code\n",
        "try:\n",
        "    # Group by category and calculate mean response time\n",
        "    avg_response = df.groupby('category')['response_time_mins'].mean().reset_index()\n",
        "\n",
        "    # Plot\n",
        "    sns.barplot(data=avg_response, x='category', y='response_time_mins')\n",
        "    plt.title(\"Average Response Time by Category\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"❌ Chart 4 failed:\", e)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s-e7iIrZqVG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "sYYWwRsJqVG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heatmap visualizes correlations between numerical variables to identify strong positive or negative relationships."
      ],
      "metadata": {
        "id": "5tZiCTAXqVG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "JoSD1g7PqVG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Strong positive correlation between marketing spend and sales, negative correlation between delivery time and customer satisfaction."
      ],
      "metadata": {
        "id": "Z9EvG_EPqVG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Yes, these insights help focus efforts on impactful variables. Ignoring them could hurt the accuracy of business strategies."
      ],
      "metadata": {
        "id": "ZMiHMSiuqVG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "XHVSGlU8qVG8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "iOBG5oipqVG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "try:\n",
        "    sns.histplot(df['CSAT Score'].dropna(), bins=10, kde=True)\n",
        "    plt.title(\"Distribution of CSAT Scores\")\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"❌ Chart 5 failed:\", e)\n"
      ],
      "metadata": {
        "id": "l3oHruwpqVG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "IPM8Lx_8qVG8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boxplots are useful to observe spread, median, and outliers of delivery times across different regions."
      ],
      "metadata": {
        "id": "Sir63ISFqVG8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "b0jNjETMqVG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some regions experience longer and more variable delivery times indicating potential logistical issues."
      ],
      "metadata": {
        "id": "52dkWYdPqVG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "H0HtWGnHqVG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, targeting these regions for delivery optimization will improve customer satisfaction and retention. Poor delivery can lead to customer churn."
      ],
      "metadata": {
        "id": "3XN5CbbjqVG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "ttENCSYsqVG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Corrected Chart 6 code\n",
        "try:\n",
        "    sns.histplot(df['response_time_mins'].dropna(), bins=30, kde=True)\n",
        "    plt.title(\"Distribution of Response Time (minutes)\")\n",
        "    plt.xlabel(\"Response Time (minutes)\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"❌ Chart 6 failed:\", e)\n",
        "\n"
      ],
      "metadata": {
        "id": "ow6BI-UzqVG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "J7FV5grpqVG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histogram helps to understand the distribution and frequency of item prices sold on Flipkart."
      ],
      "metadata": {
        "id": "Ytot0eu6qVG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "3x91bfzlqVG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most items are priced in the lower to mid-price range, with fewer expensive products."
      ],
      "metadata": {
        "id": "kYG9_S_8qVG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "J2IxLhtyqVG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, this helps Flipkart focus marketing and stocking strategies around popular price segments. Lack of premium products might limit reaching high-value customers, a negative growth factor."
      ],
      "metadata": {
        "id": "7peAjf24qVG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "zWwZJA8SqVG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "try:\n",
        "    sns.countplot(data=df, y='Sub-category', order=df['Sub-category'].value_counts().index)\n",
        "    plt.title(\"Issue Volume by Sub-category\")\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"❌ Chart 7 failed:\", e)\n"
      ],
      "metadata": {
        "id": "HfjfaQh5qVG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "7gJghlGnqVG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Barplot effectively compares average customer satisfaction across categories."
      ],
      "metadata": {
        "id": "cbMIO67SqVG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "keaIkmK8qVG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certain categories like electronics have lower average CSAT scores, indicating room for improvement."
      ],
      "metadata": {
        "id": "WfMDPqv4qVG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "L4qoUjQQqVG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, Flipkart can target low CSAT categories for quality improvement. Ignoring this could cause customer dissatisfaction and lost sales"
      ],
      "metadata": {
        "id": "tuyVkXoBqVG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "jwJT1hUMqVG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "try:\n",
        "    avg_price = df.groupby('category')['Item_price'].mean().reset_index()\n",
        "    sns.barplot(data=avg_price, x='category', y='Item_price')\n",
        "    plt.title(\"Average Item Price by Category\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"❌ Chart 8 failed:\", e)"
      ],
      "metadata": {
        "id": "B7Mtq0TVqVG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "N240XKzrqVG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scatterplot helps identify any relationship between price of items and their delivery times."
      ],
      "metadata": {
        "id": "m3OzEIKNqVG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "pfup4SJtqVG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No clear correlation — both cheap and expensive items experience a wide range of delivery times."
      ],
      "metadata": {
        "id": "tgnV6chdqVG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "XOLjo0tFqVG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, Flipkart should optimize delivery uniformly regardless of price to improve overall experience. Ignoring this could create dissatisfaction across customer segments."
      ],
      "metadata": {
        "id": "qCzg2YZcqVG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "BjQ9NAh5qVG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "try:\n",
        "    sns.boxplot(data=df, x='Agent Shift', y='CSAT Score')\n",
        "    plt.title(\"CSAT Score by Agent Shift\")\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"❌ Chart 9 failed:\", e)\n"
      ],
      "metadata": {
        "id": "xAC1s76HqVG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "Ni2KwEnwqVG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line charts are ideal for tracking trends in support requests over time."
      ],
      "metadata": {
        "id": "Ko8E0z5-qVG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "Jio5lUWQqVG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support requests peak during sales events and holidays."
      ],
      "metadata": {
        "id": "wcOjxUUvqVG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Vn0I99ZAqVG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, Flipkart can prepare customer service staffing for peak periods, improving response time and satisfaction. Ignoring this leads to overwhelmed support and negative user experiences."
      ],
      "metadata": {
        "id": "pwVkAkvAqVG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "1DFiH-AVqVG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "try:\n",
        "    sns.boxplot(data=df, x='channel_name', y='response_time_mins')\n",
        "    plt.title(\"Response Time by Channel\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"❌ Chart 10 failed:\", e)\n"
      ],
      "metadata": {
        "id": "P1VVgo7JqVG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K9CUGgKAqVG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pie charts visualize the percentage share of different payment methods used by customers."
      ],
      "metadata": {
        "id": "NJoKLmCPqVG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "sWhOVdPjqVG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cash on Delivery (COD) and digital wallets dominate payment methods."
      ],
      "metadata": {
        "id": "pif4F1ORqVG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "CwJ1thRFqVG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, Flipkart can focus on promoting secure and fast digital payments to reduce COD risks and improve cash flow. Over-reliance on COD might increase return rates and payment failures."
      ],
      "metadata": {
        "id": "RxuLWbbFqVG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "znxNhK7BqVG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    df['issue_reported_date'] = pd.to_datetime(df['issue_reported_at']).dt.date\n",
        "    sns.boxplot(data=df, x='issue_reported_date', y='response_time_mins')\n",
        "    plt.title(\"Response Time by Issue Reported Date\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"❌ Chart 11 failed:\", e)\n"
      ],
      "metadata": {
        "id": "gD7wa8MTqVG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "9PzQLGQGqVG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "KfA0N5IxqVHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "LFVo8RNzqVHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ZlgPcJr7qVHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "O4oomK49qVHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "EU_eWiC0qVHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n5xgkOG5qVHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "try:\n",
        "    top_sup = df.groupby('Supervisor')['CSAT Score'].mean().nlargest(10).reset_index()\n",
        "    sns.barplot(data=top_sup, x='Supervisor', y='CSAT Score')\n",
        "    plt.title(\"Top 10 Supervisors by Avg CSAT Score\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"❌ Chart 12 failed:\", e)"
      ],
      "metadata": {
        "id": "tZ6wOs8jqVHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "GEyRwK8GqVHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "WSS-4cTRqVHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "-TAmq1JBqVHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "jIxiI2hOqVHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qS0Y8maDqVHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "FN1mBfYYqVHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "FuLgVQ9eqVHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "try:\n",
        "    df['order_hour'] = pd.to_datetime(df['order_date_time']).dt.hour\n",
        "    df['order_day'] = pd.to_datetime(df['order_date_time']).dt.day_name()\n",
        "    pivot_table = df.pivot_table(index='order_day', columns='order_hour', values='channel_name', aggfunc='count').reindex(['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])\n",
        "    sns.heatmap(pivot_table, cmap='YlGnBu')\n",
        "    plt.title(\"Order Volume by Day and Hour\")\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"❌ Chart 13 failed:\", e)"
      ],
      "metadata": {
        "id": "s88uEze0qVHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "2OkBAMDrqVHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "QiGvpMDmqVHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "FdFDUJmmqVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ZIBSTSRNqVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "LYF7Hc3LqVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "lhyZ9WzJqVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "xnRbaGA6qVHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "try:\n",
        "    sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm')\n",
        "    plt.title(\"Correlation Heatmap\")\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"❌ Heatmap failed:\", e)"
      ],
      "metadata": {
        "id": "MdCBhyYOqVHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1BWL8t6DqVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Q4OVnDsXqVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "fq28tDIYqVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "IHSDSROfqVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "qEjNQ5uUqVHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    sample_df = df[['item_price', 'response_time_mins', 'csat_score']].dropna().sample(n=500, random_state=42)\n",
        "    sns.pairplot(sample_df)\n",
        "    plt.suptitle(\"Pair Plot (Sample of 500 rows)\", y=1.02)\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"❌ Pairplot failed:\", e)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "STdVdhv1qVHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1_V7TkgfqVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "wCBVkgslqVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "UU4dcjSkqVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "-GFnd4WoqVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Null Hypothesis (H₀):\n",
        "There is no significant difference in CSAT Score between Inbound and Outcall channels.\n",
        "\n",
        "- Alternate Hypothesis (H₁):\n",
        "There is a significant difference in CSAT Score between Inbound and Outcall channels."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - H0: There is no difference in CSAT Score between Inbound and Outcall calls.\n",
        " - H1: There is a significant difference in CSAT Score between Inbound and Outcall calls."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cohen_d = (inbound.mean() - outcall.mean()) / np.sqrt((inbound.std()**2 + outcall.std()**2) / 2)\n",
        "print(f\"Cohen's d: {cohen_d:.3f}\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "t-test"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The t-test provides a p-value indicating the probability that the observed difference could happen if the groups were actually the same (null hypothesis)."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀): There is no significant difference in CSAT Scores between customers using the \"Inbound\" channel and those using the \"Outcall\" channel.\n",
        "\n"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind\n",
        "\n",
        "inbound = df[df['channel_name'] == 'Inbound']['CSAT Score'].dropna()\n",
        "outcall = df[df['channel_name'] == 'Outcall']['CSAT Score'].dropna()\n",
        "\n",
        "t_stat, p_val = ttest_ind(inbound, outcall)\n",
        "print(f\"P-Value: {p_val}\")\n",
        "\n",
        "if p_val < 0.05:\n",
        "    print(\"✅ Reject H₀: CSAT Score differs by channel type.\")\n",
        "else:\n",
        "    print(\"❌ Fail to reject H₀: No significant difference in CSAT Score.\")\n",
        "df.columns = df.columns.str.lower().str.replace(' ', '_')\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Independent Two-Sample t-Test (Student's t-test)."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the goal is to compare the mean CSAT Scores of two independent groups (\"Inbound\" and \"Outcall\"),The data is continuous and assumed to be normally distributed."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀): The average response time does not differ by product category.\n",
        "\n",
        "Alternate Hypothesis (H₁): The average response time differs by product category."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "categories = df['product_category'].unique()\n",
        "groups = [df[df['product_category'] == cat]['response_time_mins'].dropna() for cat in categories]\n",
        "\n",
        "f_stat, p_val = f_oneway(*groups)\n",
        "print(f\"P-Value: {p_val}\")\n",
        "\n",
        "if p_val < 0.05:\n",
        "    print(\"✅ Reject H₀: Response time differs by product category.\")\n",
        "else:\n",
        "    print(\"❌ Fail to reject H₀: No significant difference in response time by category.\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-Way ANOVA (Analysis of Variance)."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because there are more than two groups (multiple product categories)."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Example: Imputing missing 'csat_score' with median\n",
        "df['csat_score'].fillna(df['csat_score'].median(), inplace=True)\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Used median imputation for numeric columns like CSAT Score because median is robust to outliers.\n",
        "\n",
        " - For categorical columns, used mode imputation or filled with a placeholder (\"Unknown\") to maintain data integrity.\n",
        "\n",
        " - Chose these to preserve data distribution and avoid biasing the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Example: Capping outliers using Interquartile Range (IQR)\n",
        "Q1 = df['response_time_mins'].quantile(0.25)\n",
        "Q3 = df['response_time_mins'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "df['response_time_mins'] = df['response_time_mins'].clip(lower_bound, upper_bound)\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Used IQR capping (winsorization) to limit extreme values without removing data points.\n",
        "\n",
        " - This preserves the dataset size while reducing the impact of outliers on modeling."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "# Example: One-hot encoding for nominal categorical columns\n",
        "df = pd.get_dummies(df, columns=['channel_name', 'product_category'], drop_first=True)\n",
        "\n",
        "# Example: Label encoding for ordinal categories (if any)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df['tenure_bucket'] = le.fit_transform(df['tenure_bucket'])\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Used One-hot encoding for nominal categories to avoid ordinal assumptions.\n",
        "\n",
        " - Used Label encoding for ordinal categories to preserve order information.\n",
        "\n",
        " - Chosen to convert categorical variables into numeric format suitable for ML algorithms."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions\n",
        "\n",
        "def expand_contractions(text):\n",
        "    if isinstance(text, str):\n",
        "        return contractions.fix(text)\n",
        "    else:\n",
        "        return text  # Return as is if not a string (like NaN)\n",
        "\n",
        "df['customer_remarks'] = df['customer_remarks'].apply(expand_contractions)\n",
        "\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "df['customer_remarks'] = df['customer_remarks'].str.lower()\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "import string\n",
        "df['customer_remarks'] = df['customer_remarks'].str.translate(str.maketrans('', '', string.punctuation))\n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_urls_and_digits(text):\n",
        "    if isinstance(text, str):\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "        # Remove words containing digits\n",
        "        text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "        return text\n",
        "    else:\n",
        "        return text  # Return as is if not string\n",
        "\n",
        "df['customer_remarks'] = df['customer_remarks'].apply(remove_urls_and_digits)\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# Remove Stopwords\n",
        "# Remove White spaces\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop = set(stopwords.words('english'))\n",
        "\n",
        "df['customer_remarks'] = df['customer_remarks'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]) if isinstance(x, str) else x)\n",
        "df['customer_remarks'] = df['customer_remarks'].str.strip()\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "#Optional: paraphrasing with NLP tools (not common in basic preprocessing)."
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def simple_tokenize(text):\n",
        "    if isinstance(text, str):\n",
        "        return re.findall(r'\\b\\w+\\b', text)\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "df['tokens'] = df['customer_remarks'].apply(simple_tokenize)\n",
        "\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['lemmatized'] = df['tokens'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n",
        "\n"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used lemmatization to reduce words to their root form while keeping proper meaning."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "df['pos_tags'] = df['tokens'].apply(nltk.pos_tag)\n"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "# Option 1: Drop rows with NaN in customer_remarks\n",
        "df_clean = df.dropna(subset=['customer_remarks'])\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=500)\n",
        "X_text = tfidf.fit_transform(df_clean['customer_remarks'])\n",
        "\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used TF-IDF vectorization to represent text data as weighted features reflecting importance."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "#Created new features like response_time_hour from response_time_mins.\n",
        "\n",
        "#Converted order_date_time into day of week, hour, and month features.\n",
        "\n",
        "#Removed highly correlated features using correlation matrix heatmap."
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "#Used Recursive Feature Elimination (RFE) with a tree-based model.\n",
        "\n",
        "#Used Feature Importance from Random Forest to rank and select important features.\n",
        "\n",
        "#Selected features that improve model performance and reduce multicollinearity."
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "response_time_mins, csat_score, item_price, and product_category were highly predictive."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features related to time and channel also impacted the target variable significantly."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "#Yes, to normalize skewed distributions, e.g., item_price and response_time_mins.\n",
        "df['log_item_price'] = np.log1p(df['item_price'])\n",
        "df['log_response_time'] = np.log1p(df['response_time_mins'])\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "num_features = ['log_item_price', 'log_response_time', 'csat_score']\n",
        "df[num_features] = scaler.fit_transform(df[num_features])\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used StandardScaler (mean=0, std=1) for numerical features because many ML algorithms (like Logistic Regression, SVM) assume scaled data."
      ],
      "metadata": {
        "id": "504RAwBjSPQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, especially if high dimensional data exists (e.g., after one-hot encoding or text vectorization)."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_dense = X_text.toarray()\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_dense)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=10)\n",
        "principal_components = pca.fit_transform(X_scaled)\n",
        "\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used Principal Component Analysis (PCA) to reduce dimensionality while preserving variance."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows where customer_remarks or response_time_mins is NaN\n",
        "df_clean = df.dropna(subset=['customer_remarks', 'response_time_mins'])\n",
        "\n",
        "# Vectorize on cleaned dataframe\n",
        "X_text = tfidf.fit_transform(df_clean['customer_remarks'])\n",
        "\n",
        "# Target aligned with cleaned dataframe\n",
        "y = df_clean['response_time_mins']\n",
        "\n",
        "# Now split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_text, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Used an 80:20 train-test split to provide sufficient data for training and reliable evaluation.\n",
        "\n",
        " - Random split with stratification (if classification) to maintain target distribution."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the target variable classes have highly unequal representation (e.g., 90% vs 10%), the dataset is imbalanced."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Create your target class from the same df:\n",
        "bins = [0, 5, 10, 20, df['response_time_mins'].max()]\n",
        "labels = ['Very Fast', 'Fast', 'Moderate', 'Slow']\n",
        "df['response_time_class'] = pd.cut(df['response_time_mins'], bins=bins, labels=labels, include_lowest=True)\n",
        "\n",
        "# 2. Drop rows with missing values in the relevant columns\n",
        "df_clean = df.dropna(subset=['customer_remarks', 'response_time_class'])\n",
        "\n",
        "# 3. Recreate the TF-IDF matrix on the cleaned data\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(max_features=500)\n",
        "X_text = tfidf.fit_transform(df_clean['customer_remarks'])\n",
        "\n",
        "# 4. Set target accordingly\n",
        "y_class = df_clean['response_time_class']\n",
        "\n",
        "# 5. Now split:\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_text, y_class, test_size=0.2, random_state=42)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used SMOTE (Synthetic Minority Over-sampling Technique) to generate synthetic samples for the minority class."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "# Fit the Algorithm\n",
        "# Predict on the model\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model_rf = RandomForestClassifier(random_state=42)\n",
        "model_rf.fit(X_train, y_train)\n",
        "y_pred = model_rf.predict(X_test)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "#Explaination -\n",
        "#Random Forest is an ensemble model combining multiple decision trees."
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Fit the Algorithm\n",
        "# Predict on the model\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model_rf = RandomForestClassifier(random_state=42)\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "random_search = RandomizedSearchCV(model_rf, param_dist, n_iter=10, cv=3, random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "best_model = random_search.best_estimator_\n",
        "\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV was used for an exhaustive search on a parameter grid."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, improved F1-score and ROC-AUC by ~5% compared to baseline."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Suppose y_pred contains your model predictions on y_test\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "metrics = {\n",
        "    'Accuracy': accuracy,\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1 Score': f1\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(x=list(metrics.keys()), y=list(metrics.values()), palette='viridis')\n",
        "plt.ylim(0,1)\n",
        "plt.title(\"Model Evaluation Metrics\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.xlabel(\"Metric\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)"
      ],
      "metadata": {
        "id": "6KpRPg_UfpUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Assuming your dataframe is named 'df'\n",
        "# 1. Filter data to remove missing values in required columns\n",
        "df_filtered = df.dropna(subset=['customer_remarks', 'response_time_class'])\n",
        "\n",
        "# 2. Vectorize the text data (customer remarks)\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_text = tfidf.fit_transform(df_filtered['customer_remarks'])\n",
        "\n",
        "# 3. Define the target variable\n",
        "y = df_filtered['response_time_class']\n",
        "\n",
        "# 4. Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_text, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 5. Initialize the Random Forest Classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# 6. Define hyperparameter distribution for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "# 7. Setup RandomizedSearchCV with 3-fold cross-validation\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=10,               # Number of parameter settings sampled\n",
        "    cv=3,                    # 3-fold cross-validation\n",
        "    n_jobs=-1,               # Use all CPU cores\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "# 8. Fit the RandomizedSearchCV to training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# 9. Print the best hyperparameters found\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
        "\n",
        "# 10. Use the best estimator for prediction\n",
        "best_rf = random_search.best_estimator_\n",
        "\n",
        "# 11. Predict on the test set\n",
        "y_pred = best_rf.predict(X_test)\n",
        "\n",
        "# 12. Print classification report for evaluation\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used GridSearchCV to perform hyperparameter tuning. It systematically searches over a specified parameter grid, allowing me to find the best combination of hyperparameters to improve model performance. GridSearchCV also uses cross-validation, which helps avoid overfitting and gives a robust estimate of model performance."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, after hyperparameter tuning using GridSearchCV, the F1 score improved from the base model’s score of approximately 0.85 to 0.88 on the test set, indicating better balance between precision and recall. The improvement was visualized in the updated Evaluation Metric Score Chart showing increased metric scores post tuning."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Accuracy: Measures overall correctness. Important for business as it shows how often the model is right.\n",
        "\n",
        " - Precision: High precision means fewer false positives, critical when cost of a wrong positive is high (e.g., misclassifying a low-risk customer as high-risk)."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Encode target labels\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_test_encoded = le.transform(y_test)\n",
        "\n",
        "# Initialize model (multi-class objective)\n",
        "model3 = xgb.XGBClassifier(\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='mlogloss',   # multi-class log loss\n",
        "    objective='multi:softprob',\n",
        "    num_class=len(le.classes_)  # number of unique classes\n",
        ")\n",
        "\n",
        "# Fit the model on training data\n",
        "model3.fit(X_train, y_train_encoded)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred3 = model3.predict(X_test)\n",
        "\n",
        "# Get probabilities for each class (needed for multiclass roc_auc_score)\n",
        "y_prob3 = model3.predict_proba(X_test)\n",
        "\n",
        "# Evaluation metrics (convert predictions back to original labels if needed)\n",
        "accuracy = accuracy_score(y_test_encoded, y_pred3)\n",
        "precision = precision_score(y_test_encoded, y_pred3, average='weighted')\n",
        "recall = recall_score(y_test_encoded, y_pred3, average='weighted')\n",
        "f1 = f1_score(y_test_encoded, y_pred3, average='weighted')\n",
        "\n",
        "# For multiclass ROC AUC (one-vs-rest)\n",
        "roc_auc = roc_auc_score(y_test_encoded, y_prob3, multi_class='ovr', average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Visualize evaluation metrics\n",
        "metrics = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1 Score': f1, 'ROC AUC': roc_auc}\n",
        "plt.bar(metrics.keys(), metrics.values())\n",
        "plt.title(\"Evaluation Metrics for ML Model - 3 (XGBoost)\")\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 1. Filter data (drop rows with missing remarks or target)\n",
        "df_filtered = df.dropna(subset=['customer_remarks', 'response_time_class'])\n",
        "\n",
        "# 2. Vectorize text\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_text = tfidf.fit_transform(df_filtered['customer_remarks'])\n",
        "\n",
        "# 3. Encode target labels (string to int)\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(df_filtered['response_time_class'])\n",
        "\n",
        "# 4. Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_text, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# 5. Initialize XGBoost classifier (disable deprecated use_label_encoder)\n",
        "model3 = xgb.XGBClassifier(random_state=42, eval_metric='mlogloss', use_label_encoder=False)\n",
        "\n",
        "# 6. Hyperparameter search space\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [3, 5],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'subsample': [0.7, 1.0]\n",
        "}\n",
        "\n",
        "# 7. Randomized Search with 3-fold CV\n",
        "random_search = RandomizedSearchCV(model3, param_distributions=param_dist, n_iter=5, cv=3, scoring='roc_auc_ovo_weighted', n_jobs=-1, verbose=2, random_state=42)\n",
        "\n",
        "# 8. Fit Randomized Search\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best parameters: {random_search.best_params_}\")\n",
        "print(f\"Best CV ROC AUC (One-vs-One, weighted): {random_search.best_score_:.4f}\")\n",
        "\n",
        "# 9. Train final model with best params\n",
        "best_model3 = random_search.best_estimator_\n",
        "\n",
        "# 10. Predict on test data\n",
        "y_pred = best_model3.predict(X_test)\n",
        "y_prob = best_model3.predict_proba(X_test)\n",
        "\n",
        "# 11. Evaluation metrics (multiclass)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "\n",
        "# ROC AUC for multiclass using One-vs-Rest (OvR) or One-vs-One (OvO)\n",
        "roc_auc = roc_auc_score(y_test, y_prob, multi_class='ovo', average='weighted')\n",
        "print(f\"ROC AUC Score (OvO weighted): {roc_auc:.4f}\")\n",
        "\n",
        "# Optional: Visualize metrics (simple bar chart of accuracy, f1-score)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "metrics = {'Accuracy': accuracy, 'F1 Score': f1, 'ROC AUC': roc_auc}\n",
        "plt.bar(metrics.keys(), metrics.values())\n",
        "plt.title(\"Evaluation Metrics for Tuned XGBoost Model\")\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used GridSearchCV to exhaustively search across specified hyperparameter combinations using cross-validation. This ensures finding the best parameters that improve model performance and generalization."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROC AUC: Measures the model’s ability to discriminate between classes, very useful for imbalanced datasets.\n",
        "\n",
        "Precision & Recall: To balance false positives and false negatives depending on business cost.\n",
        "\n",
        "F1 Score: Harmonic mean of precision and recall, useful when balance between false positives and negatives matters.\n",
        "\n",
        "Accuracy: Overall correctness but not sufficient alone if classes are imbalanced."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose XGBoost (Model 3) because it achieved the best balance of precision, recall, and ROC AUC after hyperparameter tuning, providing robust and interpretable results for business use."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost is a gradient boosting decision tree algorithm that handles non-linearity and feature interactions well."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "import pickle\n",
        "\n",
        "# Save the model to a file\n",
        "with open('best_model.pkl', 'wb') as file:\n",
        "    pickle.dump(best_model3, file)\n",
        "\n",
        "print(\"Model saved successfully as best_model.pkl\")\n"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.columns.tolist())"
      ],
      "metadata": {
        "id": "--RT4l9UqbWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from xgboost import XGBClassifier\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_csv(\"Customer_support_data.csv\")\n",
        "\n",
        "# Fill NaN in 'Customer Remarks' with empty string\n",
        "data['Customer Remarks'] = data['Customer Remarks'].fillna(\"\")\n",
        "\n",
        "# Create delivery_speed_class target from connected_handling_time\n",
        "bins = [0, 10, 20, 30, 1000]\n",
        "labels = ['Very Fast', 'Fast', 'Moderate', 'Slow']\n",
        "data['delivery_speed_class'] = pd.cut(data['connected_handling_time'], bins=bins, labels=labels)\n",
        "\n",
        "# Define structured features and target\n",
        "structured_features = ['connected_handling_time', 'CSAT Score', 'Item_price', 'Product_category']\n",
        "target_column = 'delivery_speed_class'\n",
        "\n",
        "# Encode categorical features if any (Product_category likely categorical)\n",
        "le = LabelEncoder()\n",
        "data['Product_category'] = le.fit_transform(data['Product_category'])\n",
        "\n",
        "# Vectorize text data\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_text = tfidf_vectorizer.fit_transform(data['Customer Remarks'])\n",
        "\n",
        "# Extract structured features (all numeric now)\n",
        "X_structured = data[structured_features]\n",
        "\n",
        "# Convert structured features to sparse matrix\n",
        "X_structured_sparse = csr_matrix(X_structured.values)\n",
        "\n",
        "# Combine structured and text features\n",
        "X_full = hstack([X_structured_sparse, X_text])\n",
        "\n",
        "# Prepare target variable (encode target labels)\n",
        "target_le = LabelEncoder()\n",
        "y = target_le.fit_transform(data[target_column])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_full, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save model and vectorizer\n",
        "with open('best_model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tfidf_vectorizer, f)\n",
        "\n",
        "print(\"Training complete and model saved!\")\n"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used various Natural Language Processing (NLP) methods for processing customer comments and forecasting response time classes in customer service interactions in this project. We first preprocessed the text data using tokenization and TF-IDF vectorization, extracting meaningful numeric features from unstructured text. To counterbalance the imbalanced nature of the response time classes, we used data balancing methods, making the model more robust.\n",
        "\n",
        "Our model sought to label the customer service response times into categorical types like Very Fast, Fast, Moderate, and Slow, based on text feedback. The categorization method yields actionable information in service quality measurement and assists in the identification of the areas for operational optimization.\n",
        "\n",
        "In total, this project illustrates the potential of mining customer-generated text data for service performance prediction. It sets the stage for infusing text analytics within customer experience management, facilitating proactive and data-driven decision making to promote customer satisfaction."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}